{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3+\n",
    "\n",
    "# 3rd party imports (not present in the standard python library)\n",
    "# To install, pip install numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Standard python library imports\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A large dataset with 1.6 million tweets are being used to train the model\n",
    "# Due to its size, the file is not included in this repository\n",
    "# The dataset can be downloaded from https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "# File in current workspace\n",
    "glob.glob('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1', names = [\"Score\", \"Id\", \"Date\", \"Flag\", \"User\", \"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 records\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 records\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the model, our primary data points are the tweet and the score assoociated with the score\n",
    "# Score here is the sentiment where 0 = negative, 4 = positive\n",
    "# Columns that are not required are removed and the score is normalized to be in the 0 - 1 range\n",
    "\n",
    "df.drop([\"Id\", \"Date\", \"Flag\", \"User\"], axis = 1, inplace = True)\n",
    "df['Score'] = df['Score'].apply(lambda i : i / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tweet cleanup (this process takes a significant amount of time)\n",
    "# Use the df-cleaned.pickle to load a cleaned up dataframe\n",
    "# Removing stop words, @ mentions, webpages and special characters\n",
    "\n",
    "from nltk.corpus import stopwords # nltk.download('stopwords') before importing\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(tweet):\n",
    "    stage1 = [word for word in tweet.lower().split() if word not in stopwords.words('english')] # stopword removal\n",
    "    stage2 = [word[1:] if word.startswith('#') else word for word in stage1] # Hashtag symbol removal\n",
    "    stage3 = [stemmer.stem(word) for word in stage2 if not any([word.startswith('@'), word.startswith('http'), word.startswith('www')])] # @ mentions and websites removal and stemming\n",
    "    return ' '.join(stage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['TweetStripped'] = df['Tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe from pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('Pickled data/df-cleaned-final.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Perfoms the TF-IDF\n",
    "from sklearn.model_selection import train_test_split # Used to split the data into training and testing\n",
    "\n",
    "# Data is split in the ratio of 0.9 (train) : 0.1 (test)\n",
    "train_x, test_x, train_y, test_y = train_test_split(df['TweetStripped'], df['Score'], test_size = 0.1, shuffle = True)\n",
    "\n",
    "# To compare the accuracy when the raw tweet is used to train the model, the original data is split as well\n",
    "train_x2, test_x2, train_y2, test_y2 = train_test_split(df['Tweet'], df['Score'], test_size = 0.1, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the TfTfidfVectorizer\n",
    "vector = TfidfVectorizer(max_features = 10000, ngram_range = (1,2), stop_words='english')\n",
    "%time vector.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to pass it into various classifiers\n",
    "train_x_transformed = vector.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data will be trained on several models to find the one with the highest accuracy\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('NB', MultinomialNB()))\n",
    "\n",
    "### Models below for this dataset take significantly longer\n",
    "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#models.append(('KNN', KNeighborsClassifier()))\n",
    "#models.append(('CART', DecisionTreeClassifier()))\n",
    "#models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the models\n",
    "\n",
    "results = dict()\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits = 10, random_state = 9)\n",
    "    cv_results = model_selection.cross_val_score(model, train_x_transformed, train_y, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "    results[name] = cv_results\n",
    "    print('{}: Average: {}, std: {}'.format(name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking trained models\n",
    "\n",
    "with open('Pickled data/nn.pickle', 'rb') as f3:\n",
    "    NN = pickle.load(f3)\n",
    "\n",
    "with open('Pickled data/LR.pickle', 'rb') as f3:\n",
    "    LR = pickle.load(f3)\n",
    "\n",
    "with open('Pickled data/naive-bayes.pickle', 'rb') as f3:\n",
    "    NB = pickle.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models to train\n",
    "# Neural Network (Single layer with 100 units)\n",
    "# Logistic Regression\n",
    "# Multinomial Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network \n",
    "# (Note - training is suspended after seeing diminishing gain at around the 43rd iteration)\n",
    "\n",
    "NN = MLPClassifier(verbose=2)\n",
    "NN.fit(train_x_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(train_x_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive-Bayes\n",
    "\n",
    "NB = MultinomialNB()\n",
    "NB.fit(train_x_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions from various models\n",
    "\n",
    "predNN = NN.predict(vector.transform(test_x))\n",
    "predLR = LR.predict(vector.transform(test_x))\n",
    "predNB = NB.predict(vector.transform(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, prediction in zip(['Neural Network', 'Logistic Regression', 'Naive Bayes'], [predNN, predLR, predNB]):\n",
    "    print('Model: {}'.format(model))\n",
    "    print('Accuracy - {}'.format(accuracy_score(test_y, prediction)))\n",
    "    print('Confusion matrix - {}\\n'.format(confusion_matrix(test_y, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to test a tweet, defaults to LR due to its higher accuracy\n",
    "\n",
    "def predict(tweet, model = LR):\n",
    "    return model.predict(vector.transform([clean(tweet)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: positive, 0: negative\n",
    "\n",
    "print('NN: {}'.format(predict('I love math!', model = NN)))\n",
    "print('LR: {}'.format(predict('I love math!', model = LR)))\n",
    "print('NB: {}'.format(predict('I love math!', model = NB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shelve the model, vector and predict objects\n",
    "\n",
    "import shelve\n",
    "\n",
    "with shelve.open('shelve.model', 'c') as shelf:\n",
    "    shelf['model'] = LR\n",
    "    shelf['vector'] = vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
